---
title: "Crash Fatalities"
author: "JAlexandra"
date: "2025-05"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

I am looking at understanding the relationship between fatalCount (the count of fatalities associated with a crash) and the rest of the variables in the Crash_Analysis_System_(CAS)_data data set.

# Looking at the data descriptions
On https://opendata-nzta.opendata.arcgis.com/pages/cas-data-field-descriptions there are the descriptions of the variables along with their variable names in the Crash data set.

I found that the variables crashDistance, easting, northing, and roadMarkings are listed as in the data set on that page, but are not in this csv.

The variables X, Y, objectID, and crashRoadSideRoad are in the data set but are not listed on this page.

The X-coordinate is often referred to as the "Easting", it seems like a reasonable assumption to say that the easting variable is the X variable and the northing variable is the Y variable in this dataset.

However there is no obvious link between the variables crashDistance, roadMarkings, objectID and crashRoadSideRoad.

Because I cannot determine the exact meanings of the variables objectID, and crashRoadSideRoad I will be removing them from the data set.

Small discrepencies in variable names when comparing the field descriptions variable names to the data sets variable names:

The variable intersectionMidblock mentioned in the field descriptions appears to be the variable intersection in the dataset.

Likewise with roadCharacter1 and roadCharacter.

The variables fatalCount, crashSeverity, seriousInjuryCount and minorInjuryCount contain similar information and I'm choosing to drop crashSeverity, seriousInjuryCount and minorInjuryCount

There are a significant amount variables that give location data, such as X, Y, and region. For the purposes of this research much of this information is redundant, as a result I will be removing the following variables crashLocation1, crashLocation2, directionRoleDescription and tlaName.

The variables crashYear and crashFinancialYear contain similar information, and I am choosing to drop crashFinancialYear.

The variable urban is derived from the variable speedLimit, since it contains the same information I will be removing it from the data set.

# Libraries
```{r,warning = FALSE, message = FALSE}
library(readxl)
library(finalfit)
library(naniar)
library(scales)
library(psych)
library(ggcorrplot)
library(caret)
library(moments)
library(MVN)
library(reshape2)
library(ggplot2)
library(pander)
library(car)
library(MASS)
library(dplyr)
library(AER)
library(performance)
library(DHARMa)
library(tidyverse)
library(broom)
library(knitr)
library(Metrics)
```

# Loading Data
```{r}
Crash <- read.csv("Crash_Analysis_System_(CAS)_data.csv",
                  na.strings = c("", "Unknown", "Null", "Nil"))

```

Dropping unknown variables:
```{r}
Crash <- subset(Crash, select = -c(OBJECTID, crashRoadSideRoad))
```

Dropping unnecessary variables:
```{r}
Crash <- subset(Crash, select = c(-crashSeverity, -crashLocation1, -crashLocation2,
        -crashFinancialYear, -tlaName, -tlaId, -directionRoleDescription,
        -seriousInjuryCount, -minorInjuryCount, -urban))
```


# EDA
## Quantitative analysis
```{r}
pander(data.frame(
  mean = sapply(select_if(Crash, is.numeric), mean, na.rm = TRUE),
  median = sapply(select_if(Crash, is.numeric), median, na.rm = TRUE),
  iqr = sapply(select_if(Crash, is.numeric), IQR, na.rm = TRUE)
))
```
The variables are on extremely different scales, which could affect the values of regression coefficients, but will not affect the statistical significance or interpretation of the coefficients for the later regression.

```{r}
pander(summary(Crash))
```
Count data: fatalCount, bicycle, bridge, bus, carStationWagon, cliffBank, debris, ditch, fence, guardRail, houseOrBuilding, kerb, moped, motorcycle, NumberOfLanes, objectThrownOrDropped, otherObject, otherVehicleType, overBank, parkedVehicle, pedestrian, phoneBoxEtc, postOrPole, roadworks, schoolBus, slipOrFlood, strayAnimal, suv, taxi, trafficIsland, trafficSign, train, tree, truck, unknownVehicleType, vanOrUtility, vehicle, waterRiver.

A large amount of the count data variables are derived variables.

Discrete variables (not including count data): crashYear

Continuous variables: X, Y, advisorySpeed, areaUnitID, meshblockId, speedLimit, temporarySpeedLimit

All of the categorical variables are nominal (lack an inherent order).

The data set is majority made up of count data and categorical variables.

There is one logical variable in the data set named intersection it has 870753 NA's which is equal to the total amount of observations in the data set. This means that this variable has no data for any of the observations. As a result I will be removing it
```{r}
Crash <- subset(Crash, select = -c(intersection))
```

```{r}
pander(head(Crash))
```

I can see that some variables have lots of NA's such as advisorySpeed which has 836776 NA's - almost the entire variable's data is missing data.

## Data cleaning

### Converting Categorical data for regression
All categorical variables in the data set:
```{r}
colnames(select_if(Crash, is.character))
```

```{r}
sort(unique(Crash[["crashSHDescription"]]))
```
Note: crashSHDescription "Indicates where a crash is reported to have occurred on a State Highway (SH) marked ‘1’, or on another road type marked ‘2’" according to the field descriptions, but in this data set it is coded with "No" and "Yes".

Turning all categorical variables into factors for regression:
```{r}
Crash$crashSHDescription <- as.factor(Crash$crashSHDescription)
Crash$flatHill <- as.factor(Crash$flatHill)
Crash$holiday <- as.factor(Crash$holiday)
Crash$light <- as.factor(Crash$light)
Crash$region <- as.factor(Crash$region)
Crash$roadCharacter <- as.factor(Crash$roadCharacter)
Crash$roadLane <- as.factor(Crash$roadLane)
Crash$roadSurface <- as.factor(Crash$roadSurface)
Crash$streetLight <- as.factor(Crash$streetLight)
Crash$trafficControl <- as.factor(Crash$trafficControl)
Crash$weatherA <- as.factor(Crash$weatherA)
Crash$weatherB <- as.factor(Crash$weatherB)
Crash$crashDirectionDescription <- as.factor(Crash$crashDirectionDescription)

```

### Train, test split
```{r}
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(Crash), replace = TRUE, prob = c(0.8, 0.2))
Train_set  <- Crash[sample, ]
Test_set   <- Crash[!sample, ]
```

### Missing data

There are 59 variables and trying to place them onto one plot caused it to be unreadable. To resolve this it has been split into separate plots.

```{r}
# Producing a missing data plot for the data frame.
missing_plot(Train_set[,1:30], title = "Missing data by observation and variable")
```

```{r}
missing_plot(Train_set[,31:59], title = "Missing data by observation and variable")
```

```{r}
# Saving plots:
# plotA <- missing_plot(Train_set[,1:30], title = "Missing data by observation and variable")
# plotB <- missing_plot(Train_set[,31:59], title = "Missing data by observation and variable")
# ggsave(plotA, 
#       filename = "Missing data by observation and variable (variables 1 to 31).png",
#       device = "png")

#ggsave(plotB, 
#       filename = "Missing data by observation and variable (variables 31 to 59).png",
#       device = "png")
```

It seems that most variables in the Crash data set have large amounts of missing data. Particularly temporarySpeedLimit, pedestrian, the encoded WeatherB variables, the encoded holiday variables, and advisorySpeed which appear to be majority missing data.

Calculating how many variables have missing data:
```{r}
percentages <- c()
missing_percents_colnames <- c()
x = 0
for (i in colnames(Train_set)){
  if (sum(is.na(Train_set[[i]])) > 0){
    percentages <- append(percentages, prop_miss(Train_set[[i]]))
    missing_percents_colnames <- append(missing_percents_colnames, colnames(Train_set[i]))
    x = x + 1
  }
}
print(x)

# Turning proportion to percentage
percentages <- percent(percentages, accuracy = 0.01)

```

There are 55 variables with missing data out of 59.

Finding out what proportion of each variables data is missing:
```{r}
missing.table <- do.call(rbind, Map(data.frame, variable = missing_percents_colnames,
                                    percentage = percentages))
missing.table <- missing.table[rev(order(missing.table$percentage)), ]
row.names(missing.table) <- c(1:nrow(missing.table))
missing.table
```

30 variables have more than 59% of their data missing. Another 2 variables have around 30% of their data missing.

I think it would be best to drop these variables as other methods such as imputation for the missing data would be too computationally expensive given the scale of the data.

```{r}
# Dropping variables
Train_set <- Train_set %>% select(-missing.table$variable[1:32])
```

There are now have 27 variables in the data set.

Reducing the data down to complete cases only:
```{r}
Train_set <- Train_set[complete.cases(Train_set), ]
# making the dependent variable the first column
Train_set <- Train_set %>% relocate(fatalCount)
```
This causes the amount of observations I have to go from 870,753 to 844,965, a reduction of 2.96% (to 2 d.p).

# Plotting data
## Boxplots of categorical data

All categorical variables:
```{r}
colnames(select_if(Train_set, is.factor))
```
Bar plot of crashSHDescription:
```{r}
ggplot(Train_set, aes(x = crashSHDescription)) +
  geom_bar(fill = "navy") +
  ggtitle("crashSHDescription distribution") 
```
Less crashes occurred on a state highway, around 200,000 of the observations occurred on a state highway.

Bar plot of flatHill:
```{r}
ggplot(Train_set, aes(x = flatHill)) +
  geom_bar(fill = "navy") +
  ggtitle("flatHill distribution") 
```

Bar plot of light:
```{r}
ggplot(Train_set, aes(x = light)) +
  geom_bar(fill = "navy") +
  ggtitle("light distribution") 
```
There are less twilight observations, less than 50,000 of the observations occurred during twilight.


Bar plot of region:
```{r}
ggplot(Train_set, aes(x = region)) +
  geom_bar(fill = "navy") +
  coord_flip() +
  ggtitle("region distribution") 
```
The majority of car crashes occur in Auckland.

Bar plot of roadLane:
```{r}
ggplot(Train_set, aes(x = roadLane)) +
  geom_bar(fill = "navy") +
  coord_flip() +
  ggtitle("roadLane distribution") 
```
Very few off road observations

Bar plot of roadSurface:
```{r}
ggplot(Train_set, aes(x = roadSurface)) +
  geom_bar(fill = "navy") +
  ggtitle("roadSurface distribution") 
```

There are very few End of seal observations
```{r}
sum(Train_set$roadSurface == "End of seal")
```

Bar plot of weatherA:
```{r}
ggplot(Train_set, aes(x = weatherA)) +
  geom_bar(fill = "navy") +
  ggtitle("weatherA distribution") 
```
```{r}
sum(Train_set$weatherA == "Hail or Sleet")
```
There are only 141 Hail or Sleet observations
Many of the categorical variables are imbalanced

## Numerical data plots
Count plot of response variable fatalCount:
```{r}
ggplot(Train_set, aes(x = flatHill, y = fatalCount)) +
  geom_count(aes(color = after_stat(n), size = after_stat(n))) +
  guides(color = 'legend') + ggtitle("Count plot of fatalCount by flatHill") 
```
There are few observations with at least 1 death.

```{r}
sum(Train_set$fatalCount == 0)
sum(Train_set$fatalCount >= 1)

round((7968/836997)*100, 2)
```
Observations with 1 or more deaths make up only 0.95% of observations.

```{r}
ggplot(Train_set, aes(x = fatalCount)) +
  geom_histogram(fill = "navy")  + ggtitle("fatalCount distribution") 
```

Zero deaths appear to be the vast majority of car crashes, this could potentially indicate zero inflation.

```{r}
ggplot(data = Train_set,
       mapping = aes(x = crashYear, fill = region)) +
  geom_histogram(alpha = 0.5, bins = 50) +
  labs(x = "Crash year", y = "Count",
       title = "crashYear by flatHill and Region") +
  facet_grid(. ~ flatHill) +
  theme_bw()
```
More crashes occur on flat roads than hill roads for every year recorded. But that could be due to there being more flat roads overall in New Zealand than there are hill roads.

```{r}
ggplot(Train_set, aes(x = X, y = Y, color = region)) +
  geom_point() + ggtitle("X by Y and region") 
```
It appears that X and Y contain redundant information that can be modeled by region.

Count plot of response variable fatalCount:
```{r}
ggplot(Train_set, aes(x = fatalCount, y = speedLimit)) +
  geom_count() + ggtitle("Count plot of fatalCount by speedLimit") 
```
fatalCount's of 5 and above appear to only occur when speedLimit is above 90

## Correlation plot
```{r}
ggcorrplot(cor(select_if(Train_set, is.numeric)),
method = "square",
lab = TRUE,
lab_size = 1.9, 
type = "lower")
```

```{r}
findCorrelation(cor(select_if(Train_set, is.numeric)), cutoff = 0.7, names = TRUE)
```

Y is strongly correlated with areaUnitID and meshblockID with a correlation of -0.98 and -0.97 respectively.

The variable areaUnitID is strongly correlated with meshblockID with a correlation of 1.

Due to X and Y being strongly correlated with a correlation of 0.73 and that they contain redundant information already contained in region, I will be dropping X and Y.

Due to the strong correlation between these variables I will be removing the following variables: areaUnitID and meshblockId

```{r}
Train_set <- Train_set %>% select(-areaUnitID, -meshblockId, -Y, -X)
```

I now have 23 variables in the Crash data set.

## Skewness
```{r}
pander(skewness(select_if(Train_set, is.numeric)))
```

The response variable fatalCount has a skewness of 15.22 this is significant departure from the normal distribution which has a skewness of 0.

The majority of the numerical variables have a skewness value above 3. Only vanOrUtility, speedLimit, NumberOfLanes, crashYear and carStationWagon aren't skewed.

## Kurtosis
```{r}
pander(kurtosis(select_if(Train_set, is.numeric)))
```

The response variable fatalCount has a kurtosis of 373.7, which means it has a leptokurtic distribution (high peak) this is significant departure from the normal distribution where the absolute kurtosis value should not exceed 7.1.

The variables taxi, motorcycle, bus, moped, otherVehicleType, schoolBus and unknownVehicleType all have extremely high kurtosis values, indicating leptokurtic distributions.

The distributions of most of the variables seem highly skewed with high peaks.

The kurtosis and skewness of fatalCount (the response variable) indicates that the variables distribution deviates significantly from a normal distribution meaning that the assumption of normality has been violated.

Due to the non-normality of the data I will fit a generalized linear model to the data due to its robustness to non-normality (particularly when the data set is large).

Given that the response variable is discrete count data (counting the number of deaths per car crash) I will attempt to fit a poisson model to the data first.

# Feature selection:
Feature importance:
```{r}
roc_imp <- filterVarImp(x = Train_set[,-1], y = Train_set$fatalCount, nonpara = TRUE)
roc_imp <- data.frame(cbind(variable = rownames(roc_imp), score = roc_imp[,1]))
roc_imp$score <- as.double(roc_imp$score)
roc_imp <- roc_imp[order(roc_imp$score,decreasing = TRUE),]
pander(roc_imp)
```

Reducing the data set to the ten most important variables:
```{r}
Train_set2 <- Train_set %>% select(fatalCount, roc_imp$variable[1:10])
```

# Fitting Poisson Regression Model
Fitting a poisson regression model:
```{r}
model_glm_P <- glm(fatalCount ~ speedLimit + motorcycle + roadLane + truck + region + 
                     NumberOfLanes + otherVehicleType + crashSHDescription + 
                     carStationWagon + flatHill, data = Train_set2, family = poisson)
pander(summary(model_glm_P))
```

# Checking model assumptions:

I need to check if the response variable fatalCount follows a poisson distribution:

```{r}
simulationOutput <- simulateResiduals(fittedModel = model_glm_P)
plot(simulationOutput)
```

The p-values of the KS test, dispersion test and outlier test are very small, I conclude that the data is not sampled from a poisson distribution.

The dispersion test below also adds evidence to my conclusion:
```{r}
dispersiontest(model_glm_P, alternative = "greater")
```
The small p-value indicates that the data does not fit a poisson distribution. Overdispersion means the assumptions of the model are not met.

To handle the overdispersion I could fit a quasipoission distribution or a negative binomial distribution to the data instead of a poisson distribution.

Because I want to use AIC or BIC for model selection I will fit a negative binomial model, as quasi-poisson models cannot use AIC or BIC for model selection. This is due to quasi-Poisson models using quasi-likelihood rather than true likelihood.

```{r}
model_nb <- glm.nb(fatalCount ~ speedLimit + motorcycle + roadLane + truck + region + 
                     NumberOfLanes + otherVehicleType + crashSHDescription + 
                     carStationWagon + flatHill, data = Train_set2)
```

```{r}
BIC(model_glm_P, model_nb)
```
I can see that the negative binomial model is a better fit to the data than the poisson model according to BIC. Note the degrees of freedom increases because the negative binomial model has a dispersion parameter. 

Previously I found that only 0.95% of fatalCount data (the response variable) had a count different from 0.

I am going to test for zero inflation due to this:
```{r}
check_zeroinflation(model_nb)
```
The ratio of observed and predicted zeros is within the tolerance range which means that zero inflation is not an issue for the negative binomial model, so there is no need to fit a zero-inflated negative binomial model.

## Residual plots
```{r}
simulationOutput2 <- simulateResiduals(fittedModel = model_nb)
plot(simulationOutput2)
```

The KS test and the dispersion test are not statistically significant at a significance level of 0.05

The outlier test has a p-value of 0 which is concerning, this indicates that there could potentially be outliers in the data which could negatively affect the model by leading to overfitting.

## Diagnostic measures

Is there severe multicollinearity in the data set:
```{r}
pander(vif(model_nb))
```

The variance inflation factors (VIF's) are all below 10, meaning there is no evidence of severe multicollinearity of the predictors.

Cook's distance:
```{r}
influencePlot(model_nb)
```
There are six oberservations that are influential points according to cook's distance, I will remove them from the data set.

```{r}
Train_set2 <- Train_set2[-c(190331, 221131, 399903, 634541, 665208, 772429),]
```

Finding outliers using the standardized residuals:
```{r}
suppressWarnings({model.diag.metrics <- augment(model_nb)})
model.diag.metrics <- model.diag.metrics %>%
mutate(index = 1:nrow(model.diag.metrics)) %>%
select(index, everything())

large_std_resid <- model.diag.metrics[model.diag.metrics$.std.resid > 3 | model.diag.metrics$.std.resid < -3, ]
nrow(large_std_resid)
```
There are 133 observations that have standardized residuals larger than 3 in absolute value.

I am going to remove these from the data set:
```{r}
Train_set2 <- Train_set2[-c(large_std_resid$index), ]
```

# Variable selection
Refitting model using data excluding influential points and outliers:
```{r}
model_nb <- glm.nb(fatalCount ~ speedLimit + motorcycle + roadLane + truck + region + 
                     NumberOfLanes + otherVehicleType + crashSHDescription + 
                     carStationWagon + flatHill, data = Train_set2)
```

Stepwise BIC:
```{r}
step(model_nb, direction = "both", k = log(nrow(Train_set2)))
```
Stepwise BIC initially removed flatHill and otherVehicleType, but then added these variables back into the model. This suggests that the inclusion of these variables improve (decrease) the BIC of the model.

BIC stepwise regression eventually selected all variables already in the model.

# Residual plots
```{r}
simulationOutput2 <- simulateResiduals(fittedModel = model_nb)
plot(simulationOutput2)
```

The KS test and dispersion test are statistically significant at a significance level of 0.05, so I conclude that the data does not fit a negative binomial model. I may need to consider other models.

The outlier test has a p-value of 0 which is concerning, this indicates that there could potentially be outliers in the data which could negatively affect the model by leading to overfitting. It's possible that using type = binomial may have inflated the amount of estimated outliers, so I will use a bootstrap method next to determine the amount of outliers.

# Handling Outliers
```{r}
outliers <- testOutliers(simulationOutput2, type = "bootstrap", nBoot = 20)
```
```{r}
outliers
```
Outliers according to the DHARMa bootstrapped outlier test are defined as observations that fall outside the range of simulated values, meaning they have scaled residuals of 0 or 1.

There are 930 more outliers than expected in the data set.

Checking the residual plots to see if removing these outliers fixes the outlier problem:
```{r}
Potential_outliers <- which(simulationOutput2$scaledResiduals == 1 | simulationOutput2$scaledResiduals == 0)
Train_set3 <- Train_set2[-c(Potential_outliers), ]
```

```{r}
model_nb2 <- glm.nb(fatalCount ~ speedLimit + motorcycle + roadLane + truck + region + 
                     NumberOfLanes + otherVehicleType + crashSHDescription + 
                     carStationWagon + flatHill, data = Train_set3)
```

```{r}
simulationOutput3 <- simulateResiduals(fittedModel = model_nb2)
plot(simulationOutput3)
```

Removing the outliers identified by the first outlier test did not fix the outlier issue, however it did affect the KS test and Dispersion test.

The KS test now has a non-significant p-value which informs me that the negative binomial distribution does fit the data.

The dispersion test also has a non-significant p-value.

Given this result it seems that model_nb2: the negative binomial model that excludes the outliers identified using the bootstrap method is a better fit to the data than model_nb, I am going to use the MAE, MSE and RMSE to compare the models 

# Evaluation of model
Applying the same method for missing data removal on the test set:
```{r}
Test_set2 <- Test_set %>% select(-missing.table$variable[1:32])
Test_set2 <- Test_set2[complete.cases(Test_set2), ]
# making the dependent variable the first column
Test_set2 <- Test_set2 %>% relocate(fatalCount)

# Selecting variables used for the model
Test_set2 <- Test_set2 %>% select(fatalCount, roc_imp$variable[1:10])
```

```{r}
# predicting response data based off model and test data.
prediction <- predict(model_nb, Test_set2, type = "response")
prediction2 <- predict(model_nb2, newdata = Test_set2, type = "response")

MAE <- mae(actual = Test_set2$fatalCount, predicted = prediction)
MAE2 <- mae(actual = Test_set2$fatalCount, predicted = prediction2)
MSE <- mse(actual = Test_set2$fatalCount, predicted = prediction)
MSE2 <- mse(actual = Test_set2$fatalCount, predicted = prediction2)
RMSE <- rmse(actual = Test_set2$fatalCount, predicted = prediction)
RMSE2 <- rmse(actual = Test_set2$fatalCount, predicted = prediction2)

df <- data.frame(. = c("MAE", "MSE", "RMSE"),
                  model_nb = c(MAE, MSE, RMSE),
                 model_nb2 = c(MAE2, MSE2, RMSE2))

pander(df)
```

The mean absolute error is better for model_nb2 than model_nb.

The root mean squared error is worse for model_nb than model_nb2.

The improvement in MAE but worse RMSE for model_nb2 informs me that model_nb2 is making more large scale errors and fewer small scale errors than model model_nb.

Considering that the difference in RMSE is 0.0003 between the models, and the previous KS and dispersion tests on the models, I will move forward with model_nb2.

Interpretation of MAE for model_nb2:

MAE: On average the models predictions are around 0.01837 deaths away from the true death counts for a car crash.

# Interpretting model coefficients
```{r}
pander(summary(model_nb2))
```
Interpretation of coefficients:

For a one unit change in the speedLimit, the log of expected counts of fatalCount changes by 0.03749, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

For a one unit change in the motorcycle, the log of expected counts of fatalCount changes by 1.13, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

The expected log count for 2-way road lane is 1.825 higher than the expected log count for a 1-way road lane, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for Off road lane is 1.829 higher than the expected log count for a 1-way road lane, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

For a one unit change in the truck, the log of expected counts of fatalCount changes by 0.7649, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

The expected log count for the Bay of Plenty region is 0.4855 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Canterbury region is 0.3924 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Gisborne region is 0.03194 lower than the expected log count for the Auckland region given that the other predictor variables in the model are held constant. This is not statistically significant at a significance level of 0.05

The expected log count for the Hawke's Bay region is 0.2978 higher than the expected log count for the Auckland region given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Manawatū-Whanganui region is 0.34 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Marlborough region is 0.2019 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is not statistically significant at a significance level of 0.05

The expected log count for the Nelson region is 0.3173 lower than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is not statistically significant at a significance level of 0.05

The expected log count for the Northland region is 0.4413 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Otago region is 0.1847 lower than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Southland region is 0.1297 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is not statistically significant at a significance level of 0.05

The expected log count for the Taranaki region is 0.2836 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Tasman region is 0.09087 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is not statistically significant at a significance level of 0.05

The expected log count for the Waikato region is 0.3702 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the Wellington region is 0.1627 lower than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

The expected log count for the West Coast region is 0.2578 higher than the expected log count for the Auckland region, given that the other predictor variables in the model are held constant. This is statistically significant at a significance level of 0.05

For a one unit change in the NumberOfLanes, the log of expected counts of fatalCount changes by -2502, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

For a one unit change in the otherVehicleType, the log of expected counts of fatalCount changes by 0.4191, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

For a one unit change in the crashSHDescriptionYes, the log of expected counts of fatalCount changes by 0.1618, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

For a one unit change in the carStationWagon, the log of expected counts of fatalCount changes by -0.227, given that the other predictor variables in the model are held constant. This change is statistically significant at a significance level of 0.05

For a one unit change in the flatHill-Hill Road, the log of expected counts of fatalCount changes by -0.06764, given that the other predictor variables in the model are held constant. This change is not statistically significant at significance level of 0.05

# Conclusion

The coefficients of the the regions Gisborne, Marlborough, Nelson, Southland and Tasman aren't statistically significant which means that there is no statistically significant difference in the count of deaths for a car crash (fatalCount) between those regions and the reference level Auckland. However other regions have a statistically significant difference in the count of deaths for a car crash when compared to Auckland.

The variable flatHill is statistically insignificant however I will not be removing this variable from the data due to the fact that stepwise BIC keeps this variable in the model.

The variables are on extremely different scales, which could be affecting the specific values of regression coefficients, however this does not affect the statistical significance or interpretation of the coefficients.

Each observation in this data set could have a different number of people in the car at the time of the crash which will affect the total number of possible fatalities for that car crash, this is difficult to account for. Unfortunately the total number of people in the car at the time of the crash for each observation is unreported and I cannot determine a way to derive this variable from the variables in the data set. If the number of people in the car for each observation had been recorded then I would have treated this variable as an exposure variable and I would have used this variable as an offset in my model.

I started this project with the intent of understanding the relationships between fatalCount (the count of fatalities associated with a crash) and the other variables in the data set. I have fit a model that provides information about the relationship each variable has with the response (whether it is negative or positive, and whether it is statistically significant). I also found that the data follows a negative binomial distribution.
